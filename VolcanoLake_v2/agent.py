import numpy as np

class VolcanoLakeAgent:
    def __init__(
        self,
        env,
        learning_rate: float,
        initial_epsilon: float,
        epsilon_decay: float,
        final_epsilon: float,
        discount_factor: float = 0.95,
    ):
        self.q_values = np.zeros((env.observation_space.n, env.action_space.n)) # Se crea la Q-table con los estados y las acciones, siendo 16x4.
        self.lr = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = initial_epsilon
        self.epsilon_decay = epsilon_decay
        self.final_epsilon = final_epsilon
        self.training_error = []
    
    # Con el mÃ©todo get_action te devuelve un valor 0-3 segÃºn la condiciÃ³n.
    def get_action(self, env, obs: tuple[int, int, int]) -> int:
        if np.random.random() < self.epsilon: # Si el nÃºmero generado es menor que el epsilon genera un numero aleatorio 0-3 (las posibles acciones). Por lo tanto, si el epsilon es muy alto mÃ¡s probabilidad hay de explorar y viceversa.
            return env.action_space.sample()
        return int(np.argmax(self.q_values[obs])) # Si no se cumple la condiciÃ³n se toma el valor de 0-3 que mÃ¡s alto estÃ© en dicha observaciÃ³n de nuestra tabla-Q. 

    # Con el mÃ©todo update se actualiza la Q-table y el historial de errores de entrenamiento.
    def update(self, obs, action, reward, terminated, next_obs):
        future_q_value = (not terminated) * np.max(self.q_values[next_obs]) # Si el episodio terminÃ³, no hay futuro por lo que serÃ¡ 0 y si no terminÃ³ serÃ¡ 1. Por lo tanto: 1. Si no terminÃ³, future_q_value = max_a' Q(s', a') | 2. Si terminÃ³, future_q_value = 0
        td_error = reward + self.discount_factor * future_q_value - self.q_values[obs][action] # Esto calcula el error temporal (TD error), que mide cuÃ¡nto difiere la predicciÃ³n anterior de la nueva observaciÃ³n real. TD error = (ğ‘Ÿ+ğ›¾maxâ¡ğ‘„(ğ‘ â€²,ğ‘â€²))âˆ’ğ‘„(ğ‘ ,ğ‘)TD error=(r+Î³aâ€²maxQ(sâ€²,aâ€²))âˆ’Q(s,a). Si el TD error es grande, significa que el agente aprendiÃ³ algo nuevo e importante.Si es pequeÃ±o, significa que ya predijo bien la recompensa esperada.
        self.q_values[obs][action] += self.lr * td_error # Actualiza solo una celda especÃ­fica de la Q-table: la correspondiente al estado actual y la acciÃ³n que tomÃ³. ğ‘„(ğ‘ ,ğ‘)=ğ‘„(ğ‘ ,ğ‘)+ğ›¼Ã—TD errorQ(s,a)=Q(s,a)+Î±Ã—TDerror. Si la acciÃ³n resultÃ³ mejor de lo esperado aumenta el valor Q.Si fue peor disminuye el valor Q. Con el tiempo, las acciones buenas mantienen valores Q altos.
        self.training_error.append(td_error) # Guarda el error TD de cada actualizaciÃ³n, para analizarlo (por ejemplo, graficar cÃ³mo va bajando el error medio a lo largo del entrenamiento).

    def decay_epsilon(self):
        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay) # El epsilon se reduce de manera lineal
